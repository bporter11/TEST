import org.apache.spark.SparkConf;
import org.apache.spark.sql.SparkSession;

public class MinioSparkConnector {
    public static void main(String[] args) {
        SparkConf sparkConf = new SparkConf()
            .setAppName("MinioSparkConnector")
            .setMaster("local[*]")
            .set("spark.hadoop.fs.s3a.endpoint", "http://localhost:9000")
            .set("spark.hadoop.fs.s3a.access.key", "sparkaccesskey")
            .set("spark.hadoop.fs.s3a.secret.key", "sparksupersecretkey")
            .set("spark.hadoop.fs.s3a.path.style.access", "true")
            .set("spark.hadoop.fs.s3a.connection.ssl.enabled", "false");

        SparkSession spark = SparkSession.builder()
            .config(sparkConf)
            .getOrCreate();

        // Your Spark code to read/write from/to Minio
        // Example: Reading a file from Minio
        spark.read().text("s3a://test/your-file.txt").show();

        spark.stop();
    }
}


<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-aws</artifactId>
    <version>3.2.0</version>
</dependency>
<dependency>
    <groupId>com.amazonaws</groupId>
    <artifactId>aws-java-sdk-bundle</artifactId>
    <version>1.11.534</version>
</dependency>



spark-submit \
  --class MinioSparkConnector \
  --master local[*] \
  --packages org.apache.hadoop:hadoop-aws:3.4.0,com.amazonaws:aws-java-sdk-bundle:1.12.723 \
  target/your-app-1.0-SNAPSHOT.jar

import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import io.delta.tables.DeltaTable;
import static org.apache.spark.sql.functions.*;

public class ParquetToDelta {
    public static void main(String[] args) {
        // Check for input and output paths
        if (args.length < 2) {
            System.err.println("Usage: ParquetToDelta <input-path> <output-path>");
            System.exit(1);
        }

        String parquetInputPath = args[0];
        String deltaOutputPath = args[1];

        // Create Spark session with Delta support
        SparkSession spark = SparkSession.builder()
                .appName("ParquetToDelta")
                .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
                .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
                .getOrCreate();

        // Read the Parquet file into a DataFrame
        Dataset<Row> parquetDF = spark.read().parquet(parquetInputPath);

        // Write the DataFrame to a Delta table
        parquetDF.write().format("delta").save(deltaOutputPath);

        // Optionally, you can create a Delta table from the path
        DeltaTable deltaTable = DeltaTable.forPath(spark, deltaOutputPath);

        // Print schema of Delta table
        deltaTable.toDF().printSchema();

        // Stop Spark session
        spark.stop();
    }
}

<dependencies>
    <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-core_2.12</artifactId>
        <version>3.3.0</version>
    </dependency>
    <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-sql_2.12</artifactId>
        <version>3.3.0</version>
    </dependency>
    <dependency>
        <groupId>io.delta</groupId>
        <artifactId>delta-core_2.12</artifactId>
        <version>2.1.0</version>
    </dependency>
</dependencies>


